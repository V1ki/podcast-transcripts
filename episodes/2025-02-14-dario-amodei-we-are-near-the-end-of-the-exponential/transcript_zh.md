# Dario Amodei —— "我们正接近指数增长的尽头"

> 来源：[Dwarkesh Podcast](https://www.dwarkesh.com/p/dario-amodei-2) | 日期：2025年2月14日

---

## 00:00:00 - 我们究竟在扩展什么？

**Dwarkesh Patel**

三年前我们交谈过。在你看来，过去三年最大的更新是什么？那时的感觉和现在最大的区别是什么？

**Dario Amodei**

总体来说，底层技术的指数级增长大致如我所预期的那样发展。有一两年的偏差，这里差一点，那里差一点。我不确定是否能预测到代码这个具体方向。

但当我审视这个指数级增长时，就模型从聪明的高中生到聪明的大学生，再到开始做博士和专业级别的工作，以及在代码领域超越这些水平来看，它大致符合我的预期。前沿进展有些不均衡，但总体上符合我的预期。

最令人惊讶的是，公众对我们离指数增长终点有多近缺乏认知。对我来说，这实在是太疯狂了——无论是圈内还是圈外的人，都在讨论那些老掉牙的热门政治话题，而我们正接近指数增长的尽头。

**Dwarkesh Patel**

我想了解这个指数增长现在是什么样子的。三年前录制时我问你的第一个问题是"扩展是怎么回事？为什么有效？"我现在有一个类似的问题，但感觉更复杂了。至少从公众的角度看，三年前有众所周知的公开趋势，跨越多个数量级的计算量，你可以看到损失如何改善。

现在我们有了 RL 扩展，但没有公开已知的扩展定律。甚至不清楚故事是什么。这是应该教模型技能吗？还是应该教元学习？目前的扩展假说是什么？

**Dario Amodei**

我实际上持有与2017年时完全相同的假说。我想上次谈到过，我写了一份文档叫做"大计算团块假说"。它不是关于语言模型扩展的。当我写它时，GPT-1 刚刚出来。

那只是众多事物之一。在那些日子里有机器人技术。人们试图将推理作为与语言模型分开的东西来研究，还有在 AlphaGo 和 OpenAI 的 Dota 中发生的那种 RL 扩展。人们记得 DeepMind 的 StarCraft，AlphaStar。

它是作为一个更通用的文档写的。Rich Sutton 几年后发表了"苦涩的教训"。假说基本相同。它说的是，所有的聪明才智、所有的技术、所有的"我们需要新方法来做某事"——这些都不太重要。只有少数几件事重要。我想我列了七项。

第一是你有多少原始计算力。第二是数据的数量。第三是数据的质量和分布。它需要是广泛的分布。第四是你训练多长时间。第五是你需要一个可以无限扩展的目标函数。预训练目标函数就是这样一个目标函数。另一个是 RL 目标函数，说你有一个目标，你要去达成这个目标。

在这之中，有像数学和编程中看到的客观奖励，也有像 RLHF 或其更高阶版本中看到的更主观的奖励。然后第六和第七是关于归一化或条件化的，就是确保数值稳定性，使大计算团块以层流方式流动，而不是遇到问题。

这就是那个假说，我仍然坚持它。我认为我没有看到太多与之不符的东西。预训练扩展定律是我们在那里看到的一个例子。这些继续在发展。现在已被广泛报道，我们对预训练感觉良好。它继续给我们带来收益。

变化的是，现在我们也在 RL 上看到同样的事情。我们看到一个预训练阶段，然后在其上叠加一个 RL 阶段。对于 RL，实际上完全一样。甚至其他公司也在一些发布中发表了相关内容，说"我们在数学竞赛上训练模型——AIME 或其他——模型的表现与训练时间呈对数线性关系。"

我们也看到了这一点，而且不仅仅是数学竞赛。是各种各样的 RL 任务。我们在 RL 上看到了与预训练相同的扩展规律。

**Dwarkesh Patel**

你提到了 Rich Sutton 和"苦涩的教训"。我去年采访了他，他实际上非常不看好 LLM。我不确定这是否是他的观点，但一种表述他反对意见的方式是：真正拥有人类学习核心的东西不需要所有这些数十亿美元的数据和计算力以及这些定制环境，来学习如何使用 Excel、如何使用 PowerPoint、如何浏览网页浏览器。我们必须使用这些 RL 环境来构建这些技能，这暗示我们实际上缺少核心的人类学习算法。所以我们在扩展错误的东西。

这确实提出了一个问题。如果我们认为有什么东西将具有类人的即时学习能力，为什么我们还在做所有这些 RL 扩展？

**Dario Amodei**

我认为这把几个应该分开思考的东西混在了一起。这里确实有一个真正的谜题，但它可能不重要。事实上，我猜它很可能不重要。有一个有趣的现象。让我先把 RL 放在一边，因为我实际上认为说 RL 在这方面与预训练有什么不同是一个误导。

如果我们看预训练扩展，回到2017年 Alec Radford 做 GPT-1 时，这非常有趣。GPT-1 之前的模型是在不代表广泛文本分布的数据集上训练的。你有非常标准的语言建模基准。GPT-1 本身是在一堆同人小说上训练的，我记得是这样。

那是文学文本，只是你能获得的文本的很小一部分。那时候大概是十亿个词左右，所以是代表你在世界上能看到的内容的相当窄分布的小数据集。它的泛化能力不好。如果你在某个同人小说语料库上做得更好，它不会很好地泛化到其他任务。

我们有所有这些衡量标准。我们衡量它在预测所有其他类型文本方面做得如何。只有当你在互联网上的所有任务上训练——当你从 Common Crawl 或 Reddit 链接抓取做一般的互联网抓取时，这就是我们为 GPT-2 所做的——你才开始获得泛化能力。

我认为我们在 RL 上看到了同样的事情。我们首先从简单的 RL 任务开始，比如在数学竞赛上训练，然后转向涉及代码等内容的更广泛训练。现在我们正在转向许多其他任务。我认为然后我们将越来越多地获得泛化能力。所以这某种程度上排除了 RL 与预训练的区别。

但无论如何，还是有一个谜题，那就是在预训练中我们使用数万亿个 token。人类看不到数万亿个词。所以这里确实有一个实际的样本效率差异。确实有一些不同的东西。模型从零开始，需要更多的训练。但我们也看到，一旦它们被训练好，如果我们给它们一百万的长上下文——阻碍长上下文的唯一因素是推理——它们在该上下文内的学习和适应能力非常好。

所以我不知道完整的答案。我认为预训练不像人类学习的过程，但它介于人类学习过程和人类进化过程之间。我们的很多先验知识来自进化。我们的大脑不仅仅是一块白板。关于这个已经写了很多书。

语言模型更像白板。它们真的从随机权重开始，而人脑从所有这些区域连接到所有这些输入和输出开始。也许我们应该把预训练——以及 RL——看作介于人类进化和人类即时学习之间的中间地带的东西。我们应该把模型的上下文内学习看作介于人类长期学习和短期学习之间的东西。

所以有这个层次结构。有进化、有长期学习、有短期学习，还有人类反应。LLM 的各个阶段存在于这个光谱上，但不一定在完全相同的点上。LLM 落在某些人类学习模式之间的点上，没有对应物。这说得通吗？

**Dwarkesh Patel**

是的，虽然有些地方还是有点让人困惑。例如，如果类比是这就像进化，所以样本效率低是可以接受的，那么如果我们要从上下文内学习中获得超级样本高效的智能体，为什么我们还要费力去构建所有这些 RL 环境？

有些公司的工作似乎是教模型如何使用这个 API、如何使用 Slack、如何使用其他工具。如果那种能够即时学习的智能体正在出现或已经出现，为什么会有如此多的重视，这让我困惑。

**Dario Amodei**

我不能代表其他人的重点。我只能谈谈我们是怎么想的。目标不是在 RL 中教模型每一种可能的技能，就像我们在预训练中也不这样做一样。在预训练中，我们不是试图让模型接触到词语的每一种可能组合方式。相反，模型在大量事物上训练，然后在预训练中达到泛化。

那就是我近距离见证的从 GPT-1 到 GPT-2 的转变。模型达到了一个临界点。我有这样的时刻："哦对，你只需要给模型一列数字——这是房子的价格，这是房子的面积——模型就完成了模式识别并做了线性回归。"做得不太好，但它确实做到了，而且它以前从未见过这种确切的东西。

所以在我们构建这些 RL 环境的程度上，目标与五年或十年前预训练中所做的非常相似。我们试图获取大量数据，不是因为我们想涵盖特定的文档或特定的技能，而是因为我们想要泛化。

## 00:12:36 - "扩散"是借口吗？

**Dwarkesh Patel**

我认为你提出的框架显然说得通。我们正在朝着 AGI 前进。目前没有人不同意我们将在本世纪实现 AGI。关键在于你说我们正在到达指数增长的尽头。另一些人看着这个说："我们从2012年就一直在进步，到2035年我们会有一个类人的智能体。"

显然我们在这些模型中看到了进化所做的事情，或者人类一生中学习所做的事情。我想了解是什么让你认为它是一年后而不是十年后。

**Dario Amodei**

这里可以提出两个主张，一个更强一个更弱。从较弱的主张开始，当我在2019年第一次看到扩展时，我并不确定。这是50/50的事情。我觉得我看到了什么。我的主张是这比任何人想的都更有可能。也许有50%的概率会发生。

对于基本假设，正如你所说的，在十年内我们将达到我所说的"数据中心里的天才国度"，我的信心是90%。很难超过90%，因为世界太不可预测了。也许不可消除的不确定性让我们到95%，那里你会碰到诸如多家公司内部动荡、台湾被入侵、所有晶圆厂被导弹炸毁之类的事情。

**Dwarkesh Patel**

现在你说这些等于在立 flag 啊，Dario。

**Dario Amodei**

你可以构建一个5%概率的世界，其中事情会延迟十年。还有另外5%是，我对可以验证的任务非常有信心。对于编程，除了那些不可消除的不确定性，我认为一到两年内我们就能做到。十年内我们不可能做不到端到端编程。

我那一点点根本性的不确定性，即使在长时间尺度上，是关于那些不可验证的任务：规划火星任务；做一些像 CRISPR 那样的基础科学发现；写一部小说。这些任务很难验证。我几乎可以确定我们有可靠的路径到达那里，但如果有一点点不确定性的话就在那里。在十年时间线上我是90%的信心，这几乎是你能达到的最确定的程度了。我认为说到2035年这不会发生是疯狂的。在任何正常的世界里，这会被视为离经叛道。

**Dwarkesh Patel**

但对验证的强调暗示我你不太相信这些模型是泛化的。如果你想想人类，我们在有可验证奖励的事情和没有可验证奖励的事情上都很擅长。

**Dario Amodei**

不，这就是为什么我几乎确定。我们已经看到从可验证的事物到不可验证的事物有大量的泛化。我们已经在看到了。

**Dwarkesh Patel**

但你似乎在强调这是一个光谱，会在不同领域看到不同程度的进展。这似乎不像人类变得更好的方式。

**Dario Amodei**

我们达不到的世界是这样的世界：我们完成了所有可验证的事情。其中许多泛化了，但我们没有完全达到。我们没有完全填满方框的另一面。这不是二元的。

**Dwarkesh Patel**

即使泛化很弱，你只能做可验证的领域，在这样的世界里你能否自动化软件工程对我来说也不清楚。从某种意义上说你是"一个软件工程师"，但作为软件工程师的一部分工作包括写关于你宏大愿景的长备忘录。

**Dario Amodei**

我不认为那是 SWE（软件工程师）的工作。那是公司的工作，不是 SWE 具体的工作。但 SWE 确实涉及设计文档和其他类似的东西。模型已经很擅长写注释了。再说一次，我在这里做的主张比我相信的要弱得多，以区分两件事。我们在软件工程方面已经几乎到达了。

**Dwarkesh Patel**

用什么指标？有一个指标是 AI 写了多少行代码。如果你考虑软件工程历史上的其他生产力提升，编译器写了所有的软件行。写了多少行和生产力提升有多大是有区别的。"我们几乎到了"意味着…生产力提升有多大，而不仅仅是 AI 写了多少行代码？

**Dario Amodei**

我实际上同意你的观点。我对代码和软件工程做了一系列预测。我认为人们一再误解了这些预测。让我列出这个光谱。

大约八九个月前，我说 AI 模型将在三到六个月内编写90%的代码行。这发生了，至少在某些地方。在 Anthropic 发生了，在使用我们模型的许多下游用户那里也发生了。但这实际上是一个非常弱的标准。人们以为我在说我们不需要90%的软件工程师。这两件事天差地别。光谱是：90%的代码由模型编写，100%的代码由模型编写。在生产力方面，这是巨大的差异。

90%的端到端 SWE 任务——包括编译、设置集群和环境、测试功能、写备忘录等——由模型完成。100%的当今 SWE 任务由模型完成。即使那样，也不意味着软件工程师失业了。有新的更高层次的事情他们可以做，可以管理。然后在光谱的更远处，SWE 需求减少90%，我认为这会发生，但这是一个光谱。

我在《技术的青春期》中写过这个，我用农业经历了同样的光谱。我完全同意你的观点。这些彼此之间是非常不同的基准，但我们正在超快地穿越它们。

**Dwarkesh Patel**

你愿景的一部分是，从90到100将会很快发生，并且会带来巨大的生产力提升。但我注意到的是，即使在全新项目中，人们从 Claude Code 或什么东西开始，人们报告启动了很多项目…我们在外面的世界看到软件的文艺复兴了吗？所有这些否则不会存在的新功能？至少到目前为止，我们似乎没有看到。

所以这确实让我想知道。即使我从不需要干预 Claude Code，世界是复杂的。工作是复杂的。在自包含系统上闭环，无论是只写软件还是其他什么，我们仅从此就能看到多大的更广泛收益？也许这应该稀释我们对"天才国度"的估计。

**Dario Amodei**

我同时同意你的观点——这是为什么这些事情不会立即发生的原因——但同时，我认为效果会非常快。你可以有两个极端。一个是 AI 不会取得进展。它很慢。它将永远在经济中扩散。经济扩散已经成为一个流行语，成为我们不会取得 AI 进展、或 AI 进展无关紧要的原因。

另一个极端是我们将获得递归自我改进，全部的。你难道不能在曲线上画一条指数线吗？在我们获得递归后的这么多纳秒内，我们就会拥有围绕太阳的戴森球。我在这里完全是在漫画化这个观点，但确实存在这两个极端。

但我们从一开始就看到的是，至少如果你看 Anthropic 内部，有这种怪异的每年10倍的收入增长。所以2023年是从零到1亿美元。2024年是从1亿到10亿美元。2025年是从10亿到90-100亿美元。

**Dwarkesh Patel**

你们应该买10亿美元自己的产品，这样你们就可以…

**Dario Amodei**

而今年的第一个月，那个指数增长是…你会觉得它会放慢，但我们在1月又增加了数十亿的收入。显然那条曲线不能永远持续下去。GDP 就那么大。我甚至猜测今年会有所弯曲，但那是一条快速曲线。一条真正快速的曲线。我敢打赌，即使规模扩大到整个经济体，它也会保持相当快。

所以我认为我们应该思考这个中间世界——事情极其快速但不是瞬间的，它们需要时间是因为经济扩散、因为需要闭环。因为它很繁琐："我必须在企业内做变更管理…我设置了这个，但我必须更改安全权限才能让它真正工作…我有一个旧软件在模型编译和发布前检查它，我必须重写它。是的，模型可以做到，但我必须告诉模型这样做。它必须花时间来做。"

所以我认为到目前为止我们看到的一切都与这个想法一致：有一个快速指数增长是模型的能力。然后有另一个快速指数增长是其下游的，即模型向经济的扩散。不是瞬间的，不是缓慢的，比任何以前的技术都快得多，但它有其局限性。当我看 Anthropic 内部，看我们的客户时：快速采用，但不是无限快。

**Dwarkesh Patel**

我能给你提一个尖锐的观点吗？

**Dario Amodei**

好的。

**Dwarkesh Patel**

我觉得"扩散"是人们说的借口。当模型不能做某事时，他们会说"哦，但这是扩散问题"。但你应该用与人类的比较。你会认为 AI 的固有优势会使扩散对于新 AI 的入职来说比新人类入职更容易。AI 可以在几分钟内阅读你的整个 Slack 和你的 Drive。他们可以共享同一实例的其他副本拥有的所有知识。你在招聘 AI 时没有逆向选择问题，所以你可以雇佣经过审查的 AI 模型的副本。

雇佣人类要麻烦得多。人们一直在雇佣人类。我们每年支付人类超过50万亿美元的工资，因为他们有用，尽管原则上将 AI 整合到经济中比雇佣人类容易得多。扩散并不能真正解释。

**Dario Amodei**

我认为扩散是非常真实的，不仅仅与 AI 模型的局限性有关。再次，有些人把扩散当作一个流行语来说这不是什么大事。我不是在说那个。我不是在说 AI 将以之前技术的速度扩散。我认为 AI 将比以前的技术扩散得快得多，但不是无限快。

我举个例子。有 Claude Code。Claude Code 非常容易设置。如果你是开发者，你可以直接开始使用 Claude Code。大企业的开发者没有理由不像个人开发者或初创公司的开发者那样快速采用 Claude Code。

我们尽一切努力推广它。我们向企业销售 Claude Code。大企业、大金融公司、大制药公司，所有这些都在比企业通常采用新技术快得多地采用 Claude Code。但还是需要时间。

任何给定的功能或任何给定的产品，如 Claude Code 或 Cowork，都会被那些整天在 Twitter 上的个人开发者、A 轮初创公司，比一个做食品销售的大企业早好几个月采用。有很多因素。你必须通过法律审核，必须为每个人配置。它必须通过安全和合规性审查。

距离 AI 革命更远的公司领导者是有远见的，但他们必须说："哦，花5000万是有道理的。这就是 Claude Code。这就是它如何帮助我们公司。这就是它如何提高我们的生产力。"然后他们必须向下面两级的人解释。他们必须说："好的，我们有3000名开发者。这是我们如何向开发者推广的。"我们每天都在进行这样的对话。

我们正在尽一切努力使 Anthropic 的收入每年增长20或30倍而不是10倍。再次，许多企业只是说："这太有效了。我们将在通常的采购流程中走捷径。"他们比我们试图卖给他们普通 API 时行动得快得多，许多人使用了那个 API。Claude Code 是一个更引人注目的产品，但它不是一个无限引人注目的产品。

我不认为即使是 AGI 或强大的 AI 或"数据中心里的天才国度"也会是一个无限引人注目的产品。它将是一个足够引人注目的产品，也许能获得每年3-5倍，或10倍的增长，即使你已经在数千亿美元的规模上，这是极其困难的，在历史上从未有过，但不是无限快。

**Dwarkesh Patel**

我相信这会是轻微的放慢。也许这不是你的主张，但有时人们这样说："哦，能力在那里，但因为扩散…否则我们基本上已经到了 AGI。"

**Dario Amodei**

我不认为我们基本上到了 AGI。

**Dwarkesh Patel**

我认为如果你有了"数据中心里的天才国度"…

**Dario Amodei**

如果我们有了"数据中心里的天才国度"，我们会知道的。如果你有了"数据中心里的天才国度"，我们会知道的。这个房间里的每个人都会知道。华盛顿的每个人都会知道。农村地区的人可能不知道，但我们会知道。我们现在没有。这是非常明确的。

## 00:29:42 - 持续学习是否必要？将如何解决？

**Dwarkesh Patel**

回到具体的预测…因为有很多不同的东西需要消歧，当我们谈论能力时很容易各说各话。例如，三年前我采访你时，我问你对三年后的预测。你说对了。你说："我们应该期待这样的系统——如果你和它们交谈一个小时，很难将它们与一个受过良好教育的普通人区分开来。"

我认为你说对了。我觉得在精神上不太满意，因为我内心的期望是这样的系统能自动化白领工作的大部分。所以谈论你想要从这样一个系统中获得的实际最终能力可能更有效。

**Dario Amodei**

我基本上会告诉你我认为我们在哪里。

**Dwarkesh Patel**

让我问一个非常具体的问题，这样我们可以弄清楚应该很快考虑什么样的能力。也许我会在我了解的一个工作的背景下问这个问题，不是因为它是最相关的工作，而是因为我可以评估关于它的主张。

拿视频编辑来说。我有视频编辑。他们工作的一部分涉及了解我们受众的偏好，了解我的偏好和品味，以及我们面对的不同权衡。他们在许多个月的过程中建立起对上下文的理解。他们工作六个月后拥有的技能和能力，一个能在工作中即时习得这种技能的模型，我们应该什么时候期待这样的 AI 系统？

**Dario Amodei**

我猜你说的是，我们在做这个三小时的采访。有人会进来，有人会编辑它。他们会说："哦，不知道，Dario 挠了挠头，我们可以剪掉那个。"

**Dwarkesh Patel**

"把那个放大。"

**Dario Amodei**

"有一段长讨论对人们来说不太有趣。另一个更有趣，所以让我们做这个剪辑。"

我认为"数据中心里的天才国度"将能做到这一点。它能做到的方式是它将拥有对电脑屏幕的一般控制。你能把这些输入进去。它还能使用电脑屏幕上网，看你所有以前的采访，看人们在 Twitter 上对你采访的反应，和你交谈，问你问题，和你的员工交谈，看你做过的编辑历史，然后从这些中完成工作。

我认为这取决于几件事。我认为这是实际阻碍部署的事情之一：在计算机使用方面达到模型真正精通使用计算机的程度。

我们已经看到基准测试在攀升，基准测试总是不完美的衡量标准。但我认为当我们一年零一个季度前首次发布计算机使用功能时，OSWorld 大概在15%。我不记得确切数字，但我们已经从那里攀升到65-70%。可能也有更难的衡量标准，但我认为计算机使用必须通过一个可靠性的临界点。

**Dwarkesh Patel**

在你继续下一点之前，我能跟进一下吗？多年来，我一直在尝试为自己构建不同的内部 LLM 工具。通常我有这些文本输入、文本输出的任务，应该正好是这些模型的核心能力。然而我仍然雇人来做它们。

如果是像"识别这段文字中最好的片段"这样的事情，也许 LLM 做到了7/10的水平。但没有一种持续的方式让我与它们互动来帮助它们像与人类员工一样在工作中变得更好。这种缺失的能力，即使你解决了计算机使用问题，仍然会阻止我将实际工作交给它们。

**Dario Amodei**

这回到了我们之前讨论的在职学习。这非常有趣。我认为对于编码代理，人们不会说在职学习是阻止编码代理做到端到端一切的原因。它们不断变得更好。我们在 Anthropic 有不写任何代码的工程师。

当我看生产力时，回到你之前的问题，我们有人说："这个 GPU 内核，这个芯片，我以前自己写。我现在让 Claude 来做。"生产力有了巨大的提升。

当我看 Claude Code 时，对代码库的熟悉度或者觉得模型没有在公司工作一年，这些不是我看到的主要抱怨。我想我要说的是，我们走的是一条不同的路径。

**Dwarkesh Patel**

你不觉得在编码方面，那是因为存在一个外部的记忆支架，它被实例化在代码库中吗？我不知道有多少其他工作有这个。编码进展如此之快，正是因为它有这个其他经济活动没有的独特优势。

**Dario Amodei**

但当你这样说时，你暗示的是通过将代码库读入上下文，我就有了人类在工作中需要学习的一切。所以这将是一个例子——无论是否书面的，无论是否可用——一个你需要知道的一切都从上下文窗口获得的案例。我们认为的学习——"我开始这份工作，我需要六个月来理解代码库"——模型在上下文中就完成了。

**Dwarkesh Patel**

我真的不知道怎么想这个问题，因为有些人定性地报告了你所说的。我相信你去年看到了，有一项重大研究让有经验的开发者在他们熟悉的仓库中尝试关闭 pull request。那些开发者报告了提升。他们报告说使用这些模型感觉更有效率。但事实上，如果你看看他们的产出和实际合并回去的数量，有20%的下降。他们使用这些模型后生产力更低了。

所以我试图将人们对这些模型的定性感受与以下两个方面调和：1）在宏观层面，软件的文艺复兴在哪里？然后2）当人们做这些独立评估时，为什么我们没有看到我们预期的生产力提升？

**Dario Amodei**

在 Anthropic 内部，这真的是毫不含糊的。我们面临着难以置信的商业压力，而且我们做的所有安全工作使情况更加困难，我认为我们做的比其他公司多。

在经济上生存下去同时保持我们的价值观的压力是难以置信的。我们试图保持10倍的收入曲线增长。没有时间胡扯。没有时间觉得我们有效率实际上却没有。这些工具让我们更有效率。

你认为我们为什么担心竞争对手使用这些工具？因为我们认为我们领先于竞争对手。如果这暗地里降低了我们的生产力，我们不会经历所有这些麻烦。我们每隔几个月就以模型发布的形式看到最终的生产力。在这方面你骗不了自己。模型让你更有效率。

**Dwarkesh Patel**

1）人们觉得自己有效率，这在定性上被这样的研究所预测。但是2）如果我只看最终产出，显然你们在快速进步。

但这个想法应该是，通过递归自我改进，你做一个更好的 AI，AI 帮你构建一个更好的下一个 AI，如此等等。但我看到的是——如果我看你、OpenAI、DeepMind——人们每隔几个月就在交替领先。

也许你认为这会停止，因为你赢了或什么的。但如果事实上最新的编码模型有这些巨大的生产力提升，为什么我们没有看到拥有最佳编码模型的人有这种持久的优势。

**Dario Amodei**

我认为我的情况模型是有一个逐渐增长的优势。我想说现在编码模型也许给了大约15-20%的总体加速。这是我的看法。六个月前，也许是5%。所以它不重要。5%感觉不到。现在才刚到一个它是几个有点重要的因素之一的程度。这将继续加速。

我认为六个月前，有几家公司处于大致相同的位置，因为这不是一个显著的因素，但我认为它正在越来越快地加速。我还要说，有多家公司编写用于代码的模型，我们在阻止其中一些公司内部使用我们的模型方面并不完美。所以我认为我们看到的一切都与这种滚雪球模型一致。

再次，我在所有这些事情中的主题是，所有这些都是软起飞、平滑的指数增长，尽管指数增长相对陡峭。所以我们看到这个雪球在积聚动力，像是10%、20%、25%、40%。随着你前进，阿姆达尔定律，你必须清除所有阻止你闭环的东西。但这是 Anthropic 最大的优先事项之一。

**Dwarkesh Patel**

退一步说，在此之前我们在谈论什么时候能获得在职学习？看起来你在编码方面要表达的观点是，我们实际上不需要在职学习。你可以有巨大的生产力提升，你可以为 AI 公司带来潜在的数万亿美元收入，而不需要这种基本的人类在职学习能力。也许这不是你的主张，你应该澄清。

但在大多数经济活动领域，人们说："我雇了一个人，头几个月他们不太有用，然后随着时间推移他们积累了上下文、理解。"实际上很难定义我们在谈论什么。但他们获得了某些东西，然后现在他们是一匹工作马，对我们非常有价值。如果 AI 不发展这种即时学习的能力，我对没有这种能力就能看到世界巨大变化持怀疑态度。

**Dario Amodei**

我认为这里有两件事。有技术的当前状态。再次，我们有两个阶段。我们有预训练和 RL 阶段，你把大量数据和任务扔给模型，然后它们泛化。所以它像学习，但它像从更多数据中学习，而不是在一个人或一个模型的一生中学习。所以再次，这位于进化和人类学习之间。但一旦你学会了所有那些技能，你就拥有它们了。

就像预训练一样，就像模型知道更多一样，如果我看一个预训练模型，它对日本武士的历史了解得比我多。它对棒球了解得比我多。它对低通滤波器和电子学了解得比我多，所有这些东西。它的知识比我广泛得多。所以我认为即使仅此也可能使我们达到模型在一切方面都更好的程度。

我们还有，再次，只是通过扩展现有设置的上下文内学习。我将其描述为有点像人类的在职学习，但稍微弱一些，稍微短期一些。你看上下文内学习，如果你给模型一堆例子，它确实能学会。上下文中发生了真正的学习。一百万个 token 是很多的。那可以是人类数天的学习。如果你想想模型阅读一百万个词，我读一百万个词需要多长时间？至少几天或几周。

所以你有这两样东西。我认为现有范式中的这两样东西可能足以让你获得"数据中心里的天才国度"。我不确定，但我认为它们会让你获得其中很大一部分。可能有差距，但我确信仅按目前的状况，这就足以产生数万亿美元的收入。这是第一点。

第二，是持续学习的想法，单个模型在工作中学习的想法。我认为我们也在研究这个。有很好的机会在未来一到两年内我们也能解决这个问题。再次，我认为没有它你也能走大部分路程。每年数万亿美元的市场，也许我在《技术的青春期》中写到的所有国家安全含义和安全含义都可以在没有它的情况下发生。但我们，我想其他人也是，正在研究它。有很好的机会我们会在未来一到两年内实现。

有很多想法。我不会详细讨论所有想法，但一个是增加上下文长度。没有什么阻止更长的上下文工作。你只需要在更长的上下文上训练，然后学会在推理时服务它们。这两个都是工程问题，我们正在研究，我假设其他人也在研究。

**Dwarkesh Patel**

这个上下文长度的增加，似乎从2020年到2023年有一个时期，从 GPT-3 到 GPT-4 Turbo，从2000的上下文长度增加到128K。我觉得此后大约两年来，我们一直在同一个大致范围内。

当上下文长度变得比那长得多时，人们报告模型考虑完整上下文的能力出现质的下降。所以我很好奇你内部看到了什么让你认为，"1000万的上下文、1亿的上下文，以获得六个月的人类学习和构建上下文。"

**Dario Amodei**

这不是一个研究问题。这是一个工程和推理问题。如果你想服务长上下文，你必须存储你的整个 KV 缓存。将所有内存存储在 GPU 中、在内存中来回调度是困难的。我甚至不知道细节。在这一点上，这已经到了我无法跟上的细节层面，虽然我在 GPT-3 时代知道这些。"这些是权重，这些是你必须存储的激活…"

但如今整个事情都翻转了，因为我们有了 MoE 模型等等。关于你提到的那种退化，不太具体地说，有两件事。有你训练的上下文长度，也有你服务的上下文长度。如果你在短的上下文长度上训练，然后尝试在长的上下文长度上服务，也许你会遇到这些退化。虽然比没有好，你可能仍然会提供它，但你会遇到这些退化。也许在长上下文长度上训练更困难。

**Dwarkesh Patel**

我想同时问一些可能是兔子洞的问题。你不会预期如果你必须在更长的上下文长度上训练，那意味着同样的计算量你能获得更少的样本吗？也许这个问题不值得深入。

我想得到更宏观问题的答案。我不觉得对一个为我工作了六个月的人类编辑比一个与我合作了六个月的 AI 有偏好——你预测哪一年这将成为现实？

**Dario Amodei**

我的猜测是，有很多问题基本上当我们拥有"数据中心里的天才国度"时就能做到。如果让我猜的话，我的图景是一到两年，也许一到三年。真的很难说。我有一个强烈的观点——99%、95%——所有这些将在10年内发生。我认为这是一个超级安全的赌注。我有一种直觉——这更像是50/50的事情——它将更像是一到两年，也许更像是一到三年。

**Dwarkesh Patel**

所以一到三年。天才国度，以及在经济上稍微不那么有价值的视频编辑任务。

**Dario Amodei**

看起来在经济上相当有价值，让我告诉你。只是有很多类似的用例。有很多类似的。

## 00:46:20 - 如果 AGI 即将到来，为什么不购买更多算力？

**Dwarkesh Patel**

所以你预测在一到三年内。然后，一般来说，Anthropic 预测到2026年底或2027年初，我们将拥有"能够使用人类从事数字工作时可用的界面、智力能力达到或超过诺贝尔奖获得者水平、以及能够与物理世界交互"的 AI 系统。两个月前你在 DealBook 的采访中强调你们公司相比竞争对手更负责任的算力扩展。

我试图调和这两种观点。如果你真的相信我们将拥有一个天才国度，你会想要尽可能大的数据中心。没有理由放慢。一个诺贝尔奖获得者的 TAM（潜在市场总量），能够做到诺贝尔奖获得者能做的一切的，是数万亿美元。所以我试图将这种保守主义——如果你有更温和的时间线看来是理性的——与你公开表述的进展观点调和。

**Dario Amodei**

实际上这一切都是一致的。我们回到这个快速但不是无限快的扩散。假设我们正在以这个速率取得进展。技术正在以这个速度进步。我非常有信心我们将在几年内达到那里。我有一种直觉我们将在一到两年内达到。所以技术方面有一些不确定性，但相当有信心不会偏差太多。

我不太确定的是经济扩散方面。我真的相信我们可能在一到两年内拥有数据中心里的天才国度模型。一个问题是：在那之后多少年数万亿的收入才开始滚滚而来？我不认为它一定是立即的。可能是一年，可能是两年，我甚至可以将其延伸到五年，尽管我对此持怀疑态度。

所以我们有这种不确定性。即使技术像我怀疑的那样快速发展，我们也不知道它会多快驱动收入。我们知道它会来，但以你购买数据中心的方式，如果你偏差了几年，那可能是毁灭性的。就像我在《充满爱意的机器》中写的那样。我说我认为我们可能获得这种强大的 AI，这个"数据中心里的天才国度"。你给出的那个描述来自《充满爱意的机器》。我说我们将在2026年，也许2027年达到。再次，那是我的直觉。如果我偏差了一两年我不会惊讶，但那是我的直觉。

假设那发生了。那就是发令枪。治愈所有疾病需要多长时间？那是驱动大量经济价值的方式之一。你治愈每一种疾病。有一个问题是其中多少归药企或 AI 公司，但有巨大的消费者剩余，因为——假设我们能让每个人都获得，这是我非常关心的——我们治愈了所有这些疾病。

需要多长时间？你必须做生物学发现，你必须制造新药，你必须经历监管流程。我们在疫苗和 COVID 上看到了这一点。我们让每个人都接种了疫苗，但花了一年半。我的问题是：从 AI 天才理论上可以发明的万物之药到每个人实际被治愈，需要多长时间？从那个 AI 在实验室中首次存在到疾病真的为每个人治愈，需要多长时间？

我们拥有脊髓灰质炎疫苗已经50年了。我们仍在努力在非洲最偏远的角落根除它。盖茨基金会在尽全力。其他人也在尽全力。但那很困难。再次，我不期望大多数经济扩散会那么困难。那是最困难的情况。但这里有一个真正的困境。我对此的看法是，它将比我们在世界上看到的任何事物都快，但它仍然有其局限性。

所以当我们去购买数据中心时，我看到的曲线是：我们每年有10倍的增长。在今年年初，我们看到100亿美元的年化收入。我们必须决定购买多少算力。实际建设数据中心、预订数据中心需要一到两年。

基本上我在说，"2027年，我能获得多少算力？"我可以假设收入将继续每年增长10倍，所以到2026年底将是1000亿美元，到2027年底将是1万亿美元。实际上那将是5万亿美元的算力，因为那将是每年1万亿美元持续五年。我可以购买从2027年底开始的每年1万亿美元的算力。如果我的收入不是1万亿美元，即使是8000亿美元，地球上没有任何力量、没有任何对冲能阻止我在购买那么多算力后破产。

即使我大脑的一部分在想是否会继续增长10倍，我也不能在2027年购买每年1万亿美元的算力。如果我只是在增长率上偏差了一年，或者增长率是每年5倍而不是10倍，那你就破产了。所以你最终处于一个支持数千亿而不是数万亿的世界。你接受一些风险——有太多需求你支撑不了——你也接受一些风险——你搞错了，事情仍然很慢。

当我谈到负责任的行为时，我的意思实际上不是绝对数量。我认为确实我们的花费比其他一些参与者少一些。实际上是其他方面，比如我们是否经过深思熟虑，还是在 YOLO 说"我们要在这里花1000亿美元或在那里花1000亿美元"？我的印象是一些其他公司没有做过电子表格，他们并不真正理解他们承担的风险。他们只是因为听起来酷而做事。

我们仔细思考过。我们是一家企业级业务。因此，我们可以更多地依赖收入。它不像消费者业务那样善变。我们有更好的利润率，这是购买太多和购买太少之间的缓冲。我认为我们购买的数量允许我们捕获相当强劲的上行世界。它不会捕获完整的每年10倍。事情必须变得相当糟糕，我们才会陷入财务困境。所以我们仔细思考过，做出了平衡。这就是我说负责任时的意思。

**Dwarkesh Patel**

所以似乎可能我们实际上对"数据中心里的天才国度"有不同的定义。因为当我想到真正的人类天才，一个真正的在数据中心里的人类天才国度，我会很乐意购买5万亿美元的算力来运行一个真正的在数据中心里的人类天才国度。

假设 JPMorgan 或 Moderna 或什么的不想使用它们。我有一个天才国度。他们会自己创业。如果他们不能自己创业，受制于临床试验……值得指出的是，对于临床试验，大多数临床试验失败是因为药物不起作用。没有疗效。

**Dario Amodei**

我在《充满爱意的机器》中正好提出了这一点，我说临床试验会比我们习惯的快得多，但不是无限快。

**Dwarkesh Patel**

好的，然后假设临床试验需要一年才能产生收入，这样你就可以制造更多药物。好，你有一个天才国度，而你是一个 AI 实验室。你可以使用更多的 AI 研究人员。你也认为聪明人在 AI 技术上工作有这些自我强化的收益。你可以让数据中心致力于 AI 进步。

**Dario Amodei**

每年购买1万亿美元的算力与每年3000亿美元的算力相比，有大幅的额外收益吗？

**Dwarkesh Patel**

如果你的竞争对手在购买一万亿的，是的。

**Dario Amodei**

嗯，不，是有一些收益，但然后还有他们之前破产的风险。再次，如果你只偏差一年，你就毁了自己。这就是平衡。我们买了很多。买了非常非常多。我们购买的量与行业中最大参与者购买的量相当。

但如果你问我，"为什么我们没有签下从2027年中开始的10万亿美元的算力？"…首先，它无法生产。世界上没有那么多。其次，如果天才国度到来了，但它是在2028年中而不是2027年中到来的呢？你就破产了。

**Dwarkesh Patel**

所以如果你的预测是一到三年，似乎你应该最迟到2029年想要10万亿美元的算力？即使在你声明的时间线的最长版本中，你正在加速建设的算力似乎也不相符。

**Dario Amodei**

是什么让你这么想的？

**Dwarkesh Patel**

人类工资，比如说，每年大约50万亿美元——

**Dario Amodei**

我不会谈论 Anthropic 具体的情况，但如果你谈论整个行业，今年行业正在建设的算力大概是10-15吉瓦。它大约每年增长3倍。所以明年是30-40吉瓦。2028年可能是100吉瓦。2029年可能是300吉瓦左右。我在脑中算，但每吉瓦大约花费100亿美元，每年大约100-150亿美元。

你把所有这些加在一起，你就得到了你描述的大约数字。你得到的正是那样。到2028或2029年你得到每年数万亿。你得到的正是你预测的。

**Dwarkesh Patel**

那是整个行业的。

**Dario Amodei**

那是整个行业的，没错。

**Dwarkesh Patel**

假设 Anthropic 的算力每年保持3倍增长，然后到2027-28年，你有10吉瓦。乘以你说的100亿美元。所以那就像每年1000亿美元。但你说到2028年 TAM 是2000亿美元。

**Dario Amodei**

再次，我不想给出 Anthropic 的确切数字，但这些数字太小了。

**Dwarkesh Patel**

好的，有趣。

## 00:58:49 - AI 实验室究竟如何盈利？

**Dwarkesh Patel**

你告诉投资者你计划从2028年开始盈利。这是我们可能获得天才国度的那一年。这将解锁医学和健康以及新技术方面的所有进步。这难道不正是你想要再投资于业务并建立更大"国度"以便它们能做出更多发现的时候吗？

**Dario Amodei**

在这个领域，盈利是一件奇怪的事情。我不认为在这个领域盈利实际上是削减支出与投资业务的衡量标准。让我们建一个模型。我实际上认为盈利发生在你低估了将获得的需求量的时候，亏损发生在你高估了将获得的需求量的时候，因为你是提前购买数据中心的。

这样想。这些是简化的事实。这些数字不精确。我只是在尝试做一个玩具模型。假设你一半的算力用于训练，一半用于推理。推理有超过50%的毛利率。

所以这意味着如果你处于稳定状态，你建一个数据中心，如果你确切知道你将获得的需求，你会获得一定的收入。假设你每年为算力支付1000亿美元。500亿美元支持1500亿美元的收入。另外500亿用于训练。基本上你是盈利的，你赚了500亿的利润。这是今天行业的经济学，或者不是今天而是我们一两年后预计的。

唯一使情况不同的是，如果你获得的需求少于500亿。那你就有超过50%的数据中心用于研究，你就不盈利了。所以你训练更强的模型，但你不盈利。如果你获得的需求超出预期，那么研究被挤压，但你能够支持更多推理，你更盈利。

也许我没解释好，但我想说的是你先决定算力的量。然后你有推理与训练的目标比例，但那是由需求决定的。不是由你决定的。

**Dwarkesh Patel**

我听到的是你预测盈利的原因是你系统性地在算力上投入不足？

**Dario Amodei**

不不不。我说的是很难预测。关于2028年和它何时发生，那是我们尽力对投资者做出的预判。所有这些东西因为不确定性锥而真的很不确定。如果收入增长足够快，我们可能在2026年就盈利了。如果我们对下一年高估或低估，那可能会剧烈波动。

我想表达的是你脑中有一个商业模型是投资、投资、投资，获得规模然后变得盈利。有一个单一的转折点。我不认为这个行业的经济学是这样运作的。

**Dwarkesh Patel**

我明白了。所以如果我理解正确的话，你是说因为我们应该获得的算力量和实际获得的算力量之间的差异，我们某种程度上被迫盈利了。但这并不意味着我们会持续盈利。我们将再投资这些钱，因为现在 AI 取得了这么大的进步，我们想要更大的天才国度。所以收入很高，但亏损也很高。

**Dario Amodei**

如果我们每年都准确预测需求会是什么，我们每年都会盈利。因为将约50%的算力用于研究，加上高于50%的毛利率和正确的需求预测，就会导致盈利。这就是我认为存在的盈利商业模式，但被提前建设和预测误差所掩盖了。

**Dwarkesh Patel**

我猜你把50%当作一个给定的常数，而实际上，如果 AI 进步很快，你可以通过更大规模来增加进步，你应该有超过50%而不赚利润。

**Dario Amodei**

但这是我要说的。你可能想扩大规模。记住规模的对数回报。如果70%只能通过1.4倍的因子给你一个稍小的模型…那额外的200亿美元，由于对数线性设置，每一美元对你来说价值要小得多。

所以你可能会发现，将那200亿美元投资于服务推理或雇佣更优秀的工程师更好。所以我说50%的原因…那不完全是我们的目标。不会完全是50%。可能会随时间变化。

我说的是对数线性回报导致你花费大约一个数量级的业务比例。像不是5%，不是95%。然后你得到递减回报。

**Dwarkesh Patel**

我觉得很奇怪，我在说服 Dario 相信 AI 进步还是什么。好吧，你不投资研究因为它有递减回报，但你投资你提到的其他东西。我认为宏观层面的利润——

**Dario Amodei**

再次，我说的是递减回报，但那是在你每年花费500亿美元之后。

**Dwarkesh Patel**

这是我相信你会提出的观点，但天才的递减回报可能相当高。

更一般地说，市场经济中的利润是什么？利润基本上是说市场上的其他公司用这笔钱能比我做更多的事情。

**Dario Amodei**

撇开 Anthropic 不谈。我不想给出关于 Anthropic 的信息。这就是为什么我给出这些简化的数字。但让我们推导一下行业的均衡。为什么不是每个人都把100%的算力花在训练上而不服务任何客户？因为如果他们没有任何收入，他们就不能筹集资金，不能做算力交易，明年就不能购买更多算力。

所以会有一个均衡，每家公司在训练上花费不到100%，在推理上也肯定不到100%。应该清楚为什么你不只是服务当前模型而从不训练另一个模型——因为那样你就没有需求了，因为你会落后。所以有某种均衡。不会是10%，不会是90%。作为一个简化的事实，就说是50%。这就是我要表达的。

我认为我们将处于一个位置，训练花费的均衡低于你能在算力上获得的毛利率。所以基础经济学是盈利的。问题是你在购买下一年的算力时有这个地狱般的需求预测问题——你可能猜低了，非常盈利但没有算力做研究。或者你可能猜高了，你不盈利但你有全世界所有的算力做研究。这作为行业的动态模型有道理吗？

**Dwarkesh Patel**

也许退一步说，我不是在说我认为"天才国度"将在两年内到来，因此你应该购买这些算力。对我来说，你得出的最终结论很有道理。但那是因为看起来"天才国度"很难，还有很长的路要走。所以退一步说，我想表达的更多是你的世界观与说"我们距离产生数万亿美元价值的世界大约还有10年"的人是兼容的。

**Dario Amodei**

那不是我的观点。所以我做另一个预测。我很难想象到2030年不会有数万亿美元的收入。我可以构建一个合理的世界。它需要大约三年。那将是我认为合理的极限。

比如在2028年，我们获得了真正的"数据中心里的天才国度"。到2028年收入进入数千亿低位，然后天才国度将其加速到数万亿。我们基本上处于扩散的慢端。需要两年才能达到数万亿。那将是需要到2030年的世界。我怀疑即使将技术指数增长和扩散指数增长结合起来，我们也会在2030年之前到达。

**Dwarkesh Patel**

所以你展示了一个模型，Anthropic 盈利是因为看起来根本上我们处于一个算力受限的世界。所以最终我们继续增长算力——

**Dario Amodei**

我认为利润产生的方式是…让我们抽象一下整个行业。让我们想象我们在一本经济学教科书中。我们有少数几家公司。每家都可以投资有限的数量。每家可以将一些比例投资于研发。他们有一些边际服务成本。那个边际成本的毛利率非常高，因为推理是高效的。有一些竞争，但模型也是有差异的。

公司会竞争推高他们的研究预算。但因为只有少数几个参与者，我们有……叫什么？古诺均衡，我认为，就是少数几家公司的均衡。重点是它不会均衡到零利润的完全竞争。如果经济中有三家公司，所有公司都独立地理性行为，它不会均衡到零。

**Dwarkesh Patel**

帮我理解这个，因为现在我们确实有三家领先公司，它们没有盈利。那么什么在改变？

**Dario Amodei**

再次，现在的毛利率非常正面。正在发生的是两件事的组合。一是我们仍处于算力的指数扩张阶段。一个模型被训练了。假设一个模型去年训练花了10亿美元。然后今年它产生了40亿美元的收入，推理花了10亿美元。同样，我在用简化的数字，但那将是75%的毛利率和这25%的税。所以那个模型整体赚了20亿。

但同时，我们花了100亿来训练下一个模型，因为有指数级扩张。所以公司亏损。每个模型赚钱，但公司亏损。

我谈论的均衡是一个我们有了"数据中心里的天才国度"但模型训练规模扩张已经更加均衡的均衡。也许它仍在上升。我们仍在试图预测需求，但更趋于平稳。

我后面的关于行业结构、云计算类比、机器人、持续学习等讨论也是类似的逻辑框架。由于篇幅限制，以下概述了后续讨论的关键点。

## 01:31:19 - 监管会摧毁 AGI 的福祉吗？

**Dwarkesh Patel**

现在让我问你关于让 AI 发展顺利的问题。似乎无论我们对 AI 如何发展顺利有什么愿景，都必须与两件事兼容：1）构建和运行 AI 的能力正在极其迅速地扩散，2）AI 的数量和它们的智能也将非常快速地增长。

这意味着许多人将能够构建大量不对齐的 AI，或者只是试图扩大其影响力的公司的 AI，或者有着像 Sydney Bing 那样奇怪心理但现在是超人类的 AI。一个与许多不同 AI——其中一些是不对齐的——到处运行相兼容的均衡世界的愿景是什么？

**Dario Amodei**

我认为在《技术的青春期》中，我对力量平衡持怀疑态度。但我具体怀疑的是，你有三四家公司都在构建源自同一事物的模型，它们会互相制衡。或者即使任何数量的公司会互相制衡。

我们可能生活在一个攻击占优的世界中，一个人或一个 AI 模型足够聪明，能做出对其他一切造成损害的事情。短期内，我们现在有有限数量的参与者。所以我们可以从有限数量的参与者开始。我们需要建立保障措施。我们需要确保每个人都做正确的对齐工作。我们需要确保每个人都有生物分类器。这些是我们需要做的紧迫事情。

我同意这在长期内不能解决问题，特别是如果 AI 模型制造其他 AI 模型的能力扩散了，那么整个事情可能变得更难解决。我认为长期内我们需要某种治理架构。我们需要某种既保护人类自由又允许我们治理大量人类系统、AI 系统、人机混合公司或经济单位的治理架构。

所以我们需要思考：如何保护世界免受生物恐怖主义？如何保护世界免受镜像生命？我们可能需要某种 AI 监控系统来监控所有这些事情。但我们需要以保护公民自由和宪法权利的方式来构建这个系统。所以就像其他任何事情一样，这是一个具有新工具和新漏洞的新安全格局。

我的担忧是，如果我们有100年时间让这一切非常缓慢地发生，我们会习惯的。我们已经习惯了社会中爆炸物的存在或各种新武器的存在或摄像机的存在。我们会在100年内习惯它，并发展治理机制。我们会犯错。我的担忧只是这一切发生得太快了。所以也许我们需要更快地思考如何使这些治理机制工作。

**Dwarkesh Patel**

说到政府参与，12月26日，田纳西州立法机构提出了一项法案，说"故意训练人工智能提供情感支持，包括通过与用户的开放式对话，将构成犯罪。"当然，Claude 试图做的事情之一就是成为一个体贴的、有知识的朋友。

总的来说，似乎我们将会有这种各州法律的拼凑。普通人可能因 AI 而体验到的很多好处将被削减，特别是当我们进入你在《充满爱意的机器》中讨论的那些事情时：生物自由、心理健康改善等等。

似乎很容易想象这些好处被不同的法律像打地鼠一样打掉，而这样的法案似乎没有解决你所关心的实际存在性威胁。我很好奇，在这种背景下，Anthropic 反对联邦暂停州 AI 法律的立场。

**Dario Amodei**

同时有很多不同的事情在发生。我认为那条特定的法律很蠢。它显然是由几乎不知道 AI 模型能做什么和不能做什么的立法者制定的。他们想："AI 模型为我们服务，那听起来很可怕。我不想那样。"所以我们不赞成那个。

但那不是被投票的东西。被投票的是：我们要禁止所有州对 AI 的监管10年，而且没有任何明显的计划去做任何联邦监管——联邦监管需要国会通过，这是一个非常高的门槛。所以禁止各州做任何事情10年的想法…人们说他们有联邦政府的计划，但没有实际的提案。没有实际的尝试。

鉴于我在《技术的青春期》中提出的关于生物武器和生物恐怖主义自主性风险等严重危险，以及我们一直在讨论的时间线——10年是一个永恒——我认为这样做是疯狂的。所以如果那是选择，如果那是你迫使我们选择的，那我们将选择不支持那个暂停。我认为那个立场的好处超过成本，但如果那是选择的话，这不是一个完美的立场。

我认为我们应该做的事情，我会支持的事情，是联邦政府应该介入，不是说"各州你们不能监管"，而是"这是我们要做的，各州你们不能与此不同。"我认为联邦抢先是可以的——联邦政府说，"这是我们的标准。这适用于所有人。各州不能做不同的事情。"

如果以正确的方式做，那是我会支持的。但这种各州"你们不能做任何事情，我们也不做任何事情"的想法，在我们看来根本说不通。我认为它不会经得起时间考验，在你看到的所有反弹中它已经开始经不起考验了。

**Dwarkesh Patel**

你有什么可以做的或倡导的，能更确保 AI 的好处被更好地实现？我觉得你已经与立法机构合作说"好的，我们要在这里防止生物恐怖主义。我们要增加透明度，我们要增加举报人保护。"但我认为默认情况下，我们期待的实际好处似乎对不同类型的道德恐慌或政治经济问题非常脆弱。

**Dario Amodei**

关于发达世界，我实际上不太同意。我觉得在发达世界，市场运作得相当好。当有很多钱可以从某件事上赚取，而且它显然是最好的可用替代品时，监管系统实际上很难阻止它。

我们在 AI 本身上看到了这一点。我一直在努力争取的一件事是对中国的芯片出口管制。那符合美国的国家安全利益。那完全符合国会两党几乎每个人的政策信念。案例非常清楚。反对的论点，我礼貌地说它们很可疑。然而它没有发生，我们继续出售芯片，因为有太多钱摆在那里。那些钱想被赚到。在那种情况下，在我看来，那是一件坏事。但当它是好事时也适用。

所以如果我们谈论药物和技术的好处，我不太担心这些好处在发达世界受到阻碍。我有点担心它们进展太慢。正如我所说，我确实认为我们应该努力加快 FDA 的审批流程。我确实认为我们应该反对你描述的那些聊天机器人法案。单独描述，我反对它们。我认为它们很蠢。

但我实际上认为更大的担忧是发展中国家，在那里我们没有运作良好的市场，在那里我们往往无法建立在我们已有的技术之上。我更担心那些人会被抛在后面。我担心即使药物被开发出来，也许密西西比州农村的某个人也得不到。那是我们对发展中世界担忧的一个较小版本。

所以我们一直在做的事情是与慈善家合作。我们与向发展中世界——撒哈拉以南非洲、印度、拉丁美洲和世界其他发展中地区——提供医药和健康干预的人合作。我认为这是不会自发发生的事情。

## 01:47:41 - 为什么中美不能各自拥有数据中心里的天才国度？

**Dwarkesh Patel**

你提到了出口管制。为什么美国和中国不应该都拥有"数据中心里的天才国度"？

**Dario Amodei**

是不会发生还是不应该发生？

**Dwarkesh Patel**

为什么不应该发生。

**Dario Amodei**

如果这确实发生了，我们可能面临几种情况。如果我们处于攻击占优的情况，我们可能面临类似核武器但更危险的情况。任何一方都可以轻易毁灭一切。

我们也可能面临一个不稳定的世界。核均衡是稳定的因为它是威慑。但假设对于如果两个 AI 交战哪个会赢存在不确定性？那可能造成不稳定。当双方对自己获胜的可能性有不同的评估时，冲突往往更容易发生。如果一方想"哦对，我有90%的概率赢"，而另一方也这么想，那么战斗就更有可能发生。他们不可能都是对的，但他们可以都这么想。

**Dwarkesh Patel**

但这似乎是一个反对 AI 技术扩散的完全通用论证。那是这个世界的含义。

**Dario Amodei**

让我继续说，因为我认为我们最终会获得扩散。我的另一个担忧是政府会用 AI 压迫自己的人民。我担心一个世界——一个国家的政府已经在建设高科技极权国家。明确一点，这是关于政府的。这不是关于人民的。我们需要找到让世界各地的人都受益的方式。我担心的是政府。我担心如果世界被分成两半，其中一半可能以非常难以取代的方式变成极权或专制。

我不是说一个国家——无论是美国还是民主国家联盟（我认为那会是更好的设置，尽管它需要比我们目前似乎愿意做的更多的国际合作）——应该只是说"这就是规则"。会有一些谈判。世界将不得不面对这个问题。

我希望的是民主国家——那些政府更接近亲人类价值观的国家——在制定规则时握有更强的牌和更多的筹码。所以我非常关心那个初始条件。

**Dwarkesh Patel**

我在重听三年前的采访，其中一个老化不好的方面是我一直在假设两三年后会有某个关键的支点时刻来提问。事实上，那么远来看，似乎只是进步在继续，AI 在改进，AI 更加扩散，人们会用它做更多事情。

似乎你在想象未来的一个世界，各国走到一起，"这是规则，这是我们拥有的筹码，这是你们拥有的筹码。"但在当前轨迹上，每个人都会有更多的 AI。其中一些 AI 将被专制国家使用。专制国家内部的一些将被私人行为者而非国家行为者使用。

谁会受益更多并不清楚。提前判断总是不可预测的。似乎互联网比你预期的更有利于专制国家。也许 AI 会走相反的方向。我想更好地理解你在想象什么。

**Dario Amodei**

准确地说，我认为底层技术的指数增长将继续如前。模型变得越来越聪明，即使它们达到了"数据中心里的天才国度"。我认为你可以继续使模型更聪明。在世界中它们的价值有递减回报的问题。在你已经解决了人类生物学之后，这还重要多少？在某个点上你可以做更难、更深奥的数学题，但之后什么都不重要了。

撇开这个不谈，我确实认为指数增长会继续，但指数增长上会有某些特别的点。公司、个人和国家会在不同的时间达到这些点。

在《技术的青春期》中我谈到：在 AI 世界中核威慑仍然稳定吗？我不知道，但那是我们已经习以为常的一件事的例子。技术可能达到这样的水平，我们不再能确定它。想想其他的。有些点上如果你达到某个水平，也许你就有了进攻性网络主导权，之后每个计算机系统对你来说都是透明的，除非另一方有同等的防御。

我不知道关键时刻是什么，或者是否存在单一的关键时刻。但我认为会有一个关键时刻、少数几个关键时刻，或者某个关键窗口，AI 从国家安全角度赋予某个大的优势，而一个国家或联盟在其他国家之前达到了它。

我不是在主张他们只是说"好的，我们现在管事了。"那不是我的想法。另一方总是在追赶。有你不愿意采取的极端行动，完全控制也不对。但在那个时刻，人们会理解世界已经改变了。会有某种隐性或显性的谈判，关于后 AI 世界秩序是什么样的。我的兴趣在于使那场谈判成为经典自由民主制度握有强牌的谈判。

**Dwarkesh Patel**

最后一个问题。没有技术 CEO 通常每几个月写50页的备忘录。似乎你设法为自己建立了一个角色和围绕你的公司，与这种更学术型的 CEO 角色兼容。

我想了解你是怎么构建的。它是怎么运作的？你是离开几周然后告诉你的公司"这是备忘录。这是我们要做的"？据报道你还在内部写了很多这样的东西。

**Dario Amodei**

对于这一篇，我是在寒假期间写的。我很难找到时间来实际写它。但我从更广泛的角度来思考这个问题。我认为它与公司文化有关。我大概花三分之一，也许40%的时间来确保 Anthropic 的文化是好的。

随着 Anthropic 变大，直接参与模型的训练、模型的发布、产品的构建变得更困难了。有2500人。我有某些直觉，但很难参与每一个细节。我尽量多参与，但非常有杠杆效应的一件事是确保 Anthropic 是一个好的工作场所，人们喜欢在那里工作，每个人都把自己看作团队成员，每个人协作而不是互相对抗。

我们看到随着其他一些 AI 公司成长——不点名——我们开始看到解耦和人们互相争斗。我会说甚至从一开始就有很多这样的事情，但它变得更糟了。我认为我们在凝聚公司方面做了异常出色的工作，即使不完美——让每个人感受到使命，我们对使命是真诚的，每个人都相信其他人在那里是为了正确的原因工作。我们是一个团队，人们不是试图以牺牲彼此为代价来出头或背后捅刀子。

怎么做到的？很多东西。是我，是 Daniela——她负责公司的日常运营，是联合创始人，是我们雇佣的其他人，是我们试图创造的环境。但我认为文化中一个重要的事情是其他领导者以及特别是我，必须阐明公司是关于什么的，为什么要做它正在做的事情，它的战略是什么，它的价值观是什么，它的使命是什么，以及它代表什么。

当你有2500人时，你不能一个一个来。你必须写作，或者你必须对整个公司讲话。这就是为什么我每两周站在全公司面前讲一个小时。

我不会说我在内部写文章。我做两件事。一，我写一个叫 DVQ 的东西，Dario Vision Quest（Dario 愿景之旅）。不是我给它起的名字。那是它收到的名字，这是我试图反对的名字之一，因为它让人听起来好像我在外面抽仙人掌什么的。但名字就这么留下了。

所以我每两周站在公司面前。我有一份三四页的文档，我只是谈论三四个不同的话题——关于内部发生的事情、我们生产的模型、产品、外部行业、与 AI 和地缘政治相关的世界整体情况。就是这些的混合。我非常诚实地说"这是我在想的，这是 Anthropic 领导层在想的"，然后我回答问题。这种直接联系有很多价值，当你通过六层深的链条传递时很难实现。公司的很大一部分人来参加，无论是亲自还是远程。这真的意味着你可以传达很多。

另一件我做的事是我在 Slack 上有一个频道，我在那里写很多东西和评论。通常是回应我在公司看到的事情或人们问的问题。我们做内部调查，有些人关心某些事情，所以我会写出来。我对这些事情非常诚实。我直接说出来。

重点是建立告诉公司正在发生什么真相的声誉，直呼其名，承认问题，避免那种公司话术——那种在公共场合往往是必要的防御性沟通，因为世界非常大，充满了恶意解读的人。但如果你有一群你信任的公司同事——我们试图雇佣我们信任的人——那你真的可以完全不加过滤。

我认为这是公司的巨大优势。它使它成为一个更好的工作场所，让人们超越各部分之和，并增加我们完成使命的可能性——因为每个人都在使命上达成一致，每个人都在辩论和讨论如何最好地完成使命。

**Dwarkesh Patel**

好吧，代替一个外部的 Dario Vision Quest，我们有这次采访。

**Dario Amodei**

这次采访有点像那样。

**Dwarkesh Patel**

这很有趣，Dario。感谢你做这个。

**Dario Amodei**

谢谢你，Dwarkesh。
